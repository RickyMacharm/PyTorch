{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Gradient Descent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3IlXEJynklOKeOPB3pdbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RickyMacharm/PyTorch/blob/master/02_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycBY1mrxR4S-",
        "colab_type": "text"
      },
      "source": [
        " ## **Gradient Descent**\n",
        " In machine learning problems, we provide an input and desired output\n",
        "pair and ask our model to generalize the relationship between the given input and\n",
        "output pair. But sometimes the model learns that its predictions would be way off from the\n",
        "desired output (this difference is known as a loss)\n",
        "\n",
        "TGradient Descent is used to find the values of a function's parameters (coefficients or weights in machine learning) that minimizes the cost (or loss) function. Cost function is the difference between predictions generated by an algorithm and the actual value. \n",
        "\n",
        "Gradient Descent tries to minimize cost function such that the predictions are closer to the real values. This can be calculated in many ways and one of these is the mean square error (MSE).\n",
        "\n",
        "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat{y}_{i} \\right)^2 $$\n",
        "$y =$ actual value\n",
        "\n",
        "$\\hat{y} =$ predicted value\n",
        "\n",
        "When gradient descent is employed, the idea is to find the lowest point in this function, where mean square error is closest to zero.\n",
        "\n",
        "A gradient measures how much the output of the given function\n",
        "varies when varying the inputs by a small factor, which is the same as the concept of\n",
        "derivatives in calculus. A gradient calculates the variation in all weights with respect to the\n",
        "change in error. Gradients are the slope of a function. A higher gradient means a steeper\n",
        "slope and that a model can learn more rapidly. The gradient points toward the direction of\n",
        "steepest slope.\n",
        "\n",
        "The `Autograd` module in PyTorch performs all gradient calculations. It is the core Torch package for automatic differentiation. It often holds the value of the cost function. Using a tape-based\n",
        "system for automatic differentiation, in the forward phase, the Autograd tape will\n",
        "remember all the operations it executed, and in the backward phase it will replay them.\n",
        "\n",
        "### **Let us code**\n",
        "\n",
        "When we want to use PyTorch to create tensors to perform gradient calculations, we need to add a new key that lets\n",
        "PyTorch know what is expected.\n",
        "```python\n",
        "x = torch.full((2,3), 4, requires_grad=True)\n",
        "x\n",
        "```\n",
        "output:\n",
        "```python\n",
        "tensor([[4., 4., 4.],\n",
        "[4., 4., 4.]], requires_grad=True)\n",
        "```\n",
        "We will create another tensor, `y`, that is derived out of our initial tensor `x` above. The difference in the output of this new tensor will be seen, as it has a gradient function attached\n",
        "to it:\n",
        "```python\n",
        "y = 2*x+3\n",
        "y\n",
        "```\n",
        "output:\n",
        "```python\n",
        "tensor([[11., 11., 11.],\n",
        "[11., 11., 11.]], grad_fn=<AddBackward0>)\n",
        "```\n",
        "\n",
        "We are going to create another tensor `y`, from our initial tensor `x` using a slightly complex formula:\n",
        "```python\n",
        "y = (2*x**2+3)\n",
        "y\n",
        "```\n",
        "output:\n",
        "```python\n",
        "tensor([[35., 35., 35.],\n",
        "[35., 35., 35.]], grad_fn=<AddBackward0>)\n",
        "```\n",
        "Let us break down some concepts real quick:\n",
        "\n",
        "**`requires_grad`**: When this parameter is set to `true`, it starts tracking all the operation history and forms a backward graph for gradient calculation. For an already existing tensor `x` it can be manipulated in-place as follows: \n",
        "```python\n",
        "x.requires_grad_(True)\n",
        "```\n",
        "**`grad`**: this holds the value of gradient. If requires_grad is False it will hold a `None` value. Even if `requires_grad` is set to `True`, it will hold a None value unless `.backward()` function is called from some other node. \n",
        "\n",
        "For example, if you call `out.backward()` for some variable out that involved `y` in its calculations then `y.grad` will hold $\\partial {out}$/$\\partial{y}$.\n",
        "\n",
        "Gradients are of the output node from which `.backward()` is called.\n",
        "On turning `requires_grad = True` PyTorch will start tracking the operation and store the gradient functions at each step.\n",
        "\n",
        "Let us go back to our calculations and calculate gradients with respect to `x` on `y`, since `y` is a tensor, and\n",
        "we want to calculate the gradient with respect to this tensor. To do this, we will\n",
        "pass the shape of `x`, which is the same as `y`:\n",
        "```python\n",
        "y.backward(torch.ones_like(x))\n",
        "```\n",
        "We now output the value of the gradient of `x` using the grad attribute:\n",
        "```python\n",
        "x.grad\n",
        "```\n",
        "This results in the following:\n",
        "```python\n",
        "tensor([[16., 16., 16.],\n",
        "[16., 16., 16.]])\n",
        "```\n",
        "\n",
        "We can turn off the gradient calculation at a certain point in the code by going through the following codes. The expected outputs are given immedietely.\n",
        "\n",
        "Using the `requires_grad_()` method on the tensor:\n",
        "```python\n",
        ">> x.requires_grad\n",
        "True\n",
        "```\n",
        "to turn off and test again to see if done:\n",
        "```pyton\n",
        ">> x.requires_grad_(False) # turning of gradient\n",
        ">> x.requires_grad\n",
        "False\n",
        "```\n",
        "We can also turn off tracking the gradient calculation by using the `.no_grad()`\n",
        "method:\n",
        "```python\n",
        ">> x = torch.full((2,3), 4,requires_grad=True)\n",
        ">> x\n",
        "tensor([[4., 4., 4.],\n",
        "[4., 4., 4.]], requires_grad=True)\n",
        ">> x.requires_grad\n",
        "True\n",
        ">> with torch.no_grad():\n",
        ".. print((x**5+3).requires_grad)\n",
        "False\n",
        "```\n",
        "\n",
        "**Run all these codes in the cells to verify the outputs given herein.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi8zE90k2iMc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc34dbec-ac30-4865-b19a-4af6ec8cbadd"
      },
      "source": [
        "import torch\n",
        "x = torch.full((2,3), 4, requires_grad=True)\n",
        "x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 4., 4.],\n",
              "        [4., 4., 4.]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GbzJr2P2oh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f58d81f6-caac-43ef-d4dd-a064c84e1570"
      },
      "source": [
        "y = 2*x+3\n",
        "y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[11., 11., 11.],\n",
              "        [11., 11., 11.]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STCvwHcY21H5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}